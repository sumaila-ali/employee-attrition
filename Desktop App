import tkinter as tk
from tkinter import ttk
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import plot_tree
import numpy as np
import os
import pandas as pd
from PIL import Image, ImageTk
# -*- coding: utf-8 -*-
"""
Created on Thu June 15 17:37:49 2024

@author: Sumaila
"""

# Libraries

import pandas as pd
# Needed for data i/o
import numpy as np
# Needed for linear algebra operations
import pickle
# Needed for model export
import seaborn as sns
# Needed for data visualisation
from scipy.stats import ttest_ind,randint
# Needed for T-test
import matplotlib.pyplot as plt
# Needed for data visualisation
from PIL import Image, ImageDraw, ImageFont
# Needed for text to image conversion
from sklearn import tree
# Needed for decision tree
from sklearn.linear_model import LogisticRegression
# Needed for logistic regression
from sklearn.ensemble import RandomForestClassifier
# Needed for random forest
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# Needed for discriminant analysis
from imblearn.over_sampling import SMOTE
# TODO: Why do we need this.
from sklearn.model_selection import train_test_split
# TODO: Why do we need this.
from sklearn.metrics import precision_score, recall_score, f1_score
# TODO: Why do we need this.
from sklearn.tree import DecisionTreeClassifier, plot_tree
# Needed for decision tree
from sklearn.model_selection import train_test_split
# Needed for train-test split
from sklearn.preprocessing import MinMaxScaler
# Needed for feature scaling
from sklearn.preprocessing import StandardScaler
# Needed for Data Preprocessing. ie: Standardized Scaling
from sklearn.metrics import confusion_matrix
# Needed for confusion matrix. ie: model accuracy
from sklearn.metrics import classification_report
# Needed for classification report. ie: precision, recall, f1-score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
# Needed for parameterization. ie. determining the best set of parameters
#Â that optimizes the model outcome.
from scipy import stats
# Needed for Chi Squared test
from sklearn import metrics
# Needed for model evaluation
from sklearn.model_selection import cross_val_score
# Needed for model cross validation.
import warnings
warnings.filterwarnings('ignore')
# Needed for ignoring warnings

import xgboost as xgb
from xgboost import XGBClassifier
import os
import matplotlib.colors as mcolors
from sklearn.metrics import roc_curve, auc




def is_tree_based(model):
    return hasattr(model, "tree_") or hasattr(model, "estimators_")


def data_import(
        File_name
    ):
  df = pd.read_csv(File_name)
  data = df
  return data

def plot_performance_image(metrics, save_path='performance.png'):
    fig, ax = plt.subplots(figsize=(6, 4))
    ax.axis('off')

    text = (
        f"Train Score: {metrics['train_score']:.3f}\n"
        f"Test Score: {metrics['test_score']:.3f}\n"
        f"Precision: {metrics['precision']:.3f}\n"
        f"Recall: {metrics['recall']:.3f}\n"
        f"F1 Score: {metrics['f1']:.3f}"
    )
    ax.text(0.5, 0.5, text, fontsize=14, ha='center', va='center')
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

def plot_top_features(model, feature_names, save_path='top_features.png'):
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    elif hasattr(model, 'coef_'):
        importances = np.abs(model.coef_[0])
    else:
        return

    indices = np.argsort(importances)[::-1][:10]
    top_features = [feature_names[i] for i in indices]
    top_values = importances[indices]

    plt.figure(figsize=(8, 6))
    plt.barh(top_features[::-1], top_values[::-1], color='teal')
    plt.title('Top 10 Features')
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

def plot_tree_diagram(model, feature_names, save_path='tree_plot.png'):
    if not is_tree_based(model):
        return
    plt.figure(figsize=(10, 6))
    #plot_tree(model, max_depth=2, feature_names=feature_names, filled=True, fontsize=10)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

def plot_roc_curve(model, X_test, y_test, save_path='roc_curve.png'):
    if hasattr(model, "predict_proba"):
        probs = model.predict_proba(X_test)[:, 1]
    else:
        probs = model.decision_function(X_test)

    fpr, tpr, _ = roc_curve(y_test, probs)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

# Place this inside your class ModelInspectorApp

class ModelInspectorApp:
    def __init__(self, root, model_scores):
        self.root = root
        self.root.title("Model Performance Viewer")
        self.root.attributes('-fullscreen', True)  # Launch in full screen

        self.model_scores = model_scores

        # Dropdown and tune button
        top_frame = tk.Frame(root)
        top_frame.pack(pady=10)

        self.label = tk.Label(top_frame, text="Select a model:")
        self.label.pack(side=tk.LEFT, padx=5)

        self.combo = ttk.Combobox(top_frame, values=list(model_scores.keys()))
        self.combo.pack(side=tk.LEFT, padx=5)
        self.combo.bind("<<ComboboxSelected>>", self.update_output)

        self.tune_button = tk.Button(top_frame, text="Tune Model", command=self.tune_model)
        self.tune_button.pack(side=tk.LEFT, padx=10)

        # Container for image grid
        self.grid_frame = tk.Frame(root)
        self.grid_frame.pack(pady=20)

        # Define placeholders for canvases
        self.canvas1 = tk.Label(self.grid_frame)
        self.canvas2 = tk.Label(self.grid_frame)
        self.canvas3 = tk.Label(self.grid_frame)
        self.canvas4 = tk.Label(self.grid_frame)

        self.canvas1.grid(row=0, column=0, padx=10, pady=10)
        self.canvas2.grid(row=0, column=1, padx=10, pady=10)
        self.canvas3.grid(row=1, column=0, padx=10, pady=10)
        self.canvas4.grid(row=1, column=1, padx=10, pady=10)

    def update_output(self, event=None):
        model_name = self.combo.get()
        details = self.model_scores[model_name]

        model = details["model"]
        metrics = details["metrics"]
        feature_names = details["features"]
        X_test = details["X_test"]
        y_test = details["y_test"]

        plot_performance_image(metrics)
        plot_top_features(model, feature_names)
        if is_tree_based(model):
            plot_tree_diagram(model, feature_names)
        plot_roc_curve(model, X_test, y_test)

        self.display_image('performance.png', self.canvas1)
        self.display_image('top_features.png', self.canvas2)
        self.display_image('roc_curve.png', self.canvas3)

        if is_tree_based(model):
            self.display_image('tree_plot.png', self.canvas4)
        else:
            self.canvas4.config(image='')

    def tune_model(self):
        model_name = self.combo.get()
        if not model_name:
            return

        details = self.model_scores[model_name]
        X_train = details["X_test"]
        y_train = details["y_test"]  # For demo, using test split again
        feature_names = details["features"]

        param_grid = {
            'n_estimators': [50, 100, 150],
            'max_depth': [4, 6, 8],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'bootstrap': [True, False]
        }

        clf = RandomForestClassifier(random_state=42)
        search = RandomizedSearchCV(clf, param_distributions=param_grid,
                                    n_iter=10, cv=3, scoring='f1', n_jobs=-1, verbose=1)
        search.fit(X_train, y_train)

        tuned_model = search.best_estimator_
        y_pred = tuned_model.predict(X_train)

        tuned_metrics = {
            "train_score": tuned_model.score(X_train, y_train),
            "test_score": tuned_model.score(X_train, y_train),
            "precision": precision_score(y_train, y_pred),
            "recall": recall_score(y_train, y_pred),
            "f1": f1_score(y_train, y_pred)
        }

        self.model_scores[f"{model_name} (Tuned)"] = {
            "model": tuned_model,
            "metrics": tuned_metrics,
            "features": feature_names,
            "X_test": X_train,
            "y_test": y_train
        }

        self.combo['values'] = list(self.model_scores.keys())
        self.combo.set(f"{model_name} (Tuned)")
        self.update_output()

    def display_image(self, image_path, canvas):
        img = Image.open(image_path)
        img = img.resize((600, 400))
        tk_img = ImageTk.PhotoImage(img)
        canvas.image = tk_img
        canvas.config(image=tk_img)

    

# EXAMPLE USAGE:

df = data_import('HR data.csv')
 # List of categorical variables
categorical_variables = list(df.select_dtypes(include=['object']).columns)

# List of numerical variables
numerical_variables = df.select_dtypes(include=["int64", "float64"]).columns.tolist()

# Set the data type of the all categorical and numerical variables
for i in categorical_variables:
    df[i] = df[i].astype("category")


for j in numerical_variables:
    df[j] = df[j].astype("int64")

del[i, j]


df = pd.get_dummies(
    df, 
    columns=categorical_variables, 
    drop_first=True
)

features = df.drop(
    'Attrition', 
    axis = 1, 
    inplace = False
)
target = df['Attrition']

X_train, X_test, y_train, y_test = train_test_split(
    features, 
    target, 
    stratify = target,
    test_size = .3, 
    random_state = 44
    )


rf = RandomForestClassifier().fit(X_train, y_train)
y_pred = rf.predict(X_test)
metrics = {
    "train_score": rf.score(X_train, y_train), 
    "test_score": rf.score(X_test, y_test), 
    "precision":precision_score(y_test, y_pred), 
    "recall": recall_score(y_test, y_pred), 
    "f1": f1_score(y_test, y_pred)}

model_scores = {
     "Random Forest": {
         "model": rf,
         "metrics": metrics,
         "features": X_train.columns,
         "X_test": X_test,
         "y_test": y_test
     }
 }


root = tk.Tk()
app = ModelInspectorApp(root, model_scores)
root.mainloop()