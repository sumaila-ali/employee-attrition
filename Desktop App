import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import roc_curve, auc, precision_recall_curve, f1_score, confusion_matrix, accuracy_score, precision_score, recall_score, classification_report
import seaborn as sns
from imblearn.over_sampling import SMOTE
import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import uuid



class ModelTrainerApp:
    def __init__(self, root):
        self.root = root
        self.root.title("Model Trainer App")
        #self.root.geometry("1200x1000") # Increased height to accommodate 2x2 grid
        self.root.attributes('-fullscreen', True)

        self.df = None
        self.target_column = None
        self.model = None
        self.X_test = None
        self.y_test = None

        self.canvas_frames = [[ttk.Frame(self.root) for _ in range(2)] for _ in range(2)]
        for i in range(2):
            for j in range(2):
                self.canvas_frames[i][j].grid(row=i+1, column=j, padx=10, pady=10, sticky="nsew")
                self.root.grid_columnconfigure(j, weight=1)
                self.root.grid_rowconfigure(i+1, weight=1)

        self.canvases = [[None]*2 for _ in range(2)]

        self.create_widgets()

    def create_widgets(self):
        # File selection
        file_frame = ttk.Frame(self.root)
        file_frame.grid(row=0, column=0, columnspan=2, pady=10)
        ttk.Button(file_frame, text="Load CSV", command=self.load_csv).pack()

        # Target selection
        target_frame = ttk.Frame(self.root)
        target_frame.grid(row=10, column=0, columnspan=2, pady=10) # Moved below plots for better layout
        ttk.Label(target_frame, text="Select Target Column:").pack(side=tk.LEFT)
        self.target_menu = ttk.Combobox(target_frame, state="readonly")
        self.target_menu.pack(side=tk.LEFT)

        # Model selection
        model_frame = ttk.Frame(self.root)
        model_frame.grid(row=11, column=0, columnspan=2, pady=10)
        ttk.Label(model_frame, text="Select Model:").pack(side=tk.LEFT)
        self.model_var = tk.StringVar()
        self.model_menu = ttk.Combobox(model_frame, textvariable=self.model_var, values=["Logistic Regression", "Decision Tree", "Random Forest", "XGBoost"])
        self.model_menu.pack(side=tk.LEFT)
        self.model_menu.set("Logistic Regression") # Default model

        # Scaling option
        scale_frame = ttk.Frame(self.root)
        scale_frame.grid(row=12, column=0, columnspan=2, pady=10)
        ttk.Label(scale_frame, text="Scaling:").pack(side=tk.LEFT)
        self.scale_var = tk.StringVar()
        self.scale_menu = ttk.Combobox(scale_frame, textvariable=self.scale_var, values=["None", "StandardScaler", "MinMaxScaler"])
        self.scale_menu.current(0)
        self.scale_menu.pack(side=tk.LEFT)

        # Balancing option
        balance_frame = ttk.Frame(self.root)
        balance_frame.grid(row=13, column=0, columnspan=2, pady=10)
        self.balance_var = tk.BooleanVar()
        ttk.Checkbutton(balance_frame, text="Apply SMOTE (Balance Data)", variable=self.balance_var).pack(side=tk.LEFT)

        # Train button
        ttk.Button(self.root, text="Train Model", command=self.train_model).grid(row=14, column=0, columnspan=2, pady=20)

        # Configure grid row/column weights so plots expand
        for i in range(2):
            for j in range(2):
                self.root.grid_rowconfigure(i + 1, weight=1)
                self.root.grid_columnconfigure(j, weight=1)

    def load_csv(self):
        file_path = filedialog.askopenfilename(filetypes=[("CSV files", "*.csv")])
        if file_path:
            self.df = pd.read_csv(file_path)
            self.target_menu['values'] = self.df.columns.tolist()
            self.target_menu.current(0)
            messagebox.showinfo("Success", "CSV loaded successfully!")

    def train_model(self):
        self.target_column = self.target_menu.get()
        if self.df is None or self.df.empty or not self.target_column:
            messagebox.showerror("Error", "Please load a CSV and select a target column.")
            return

        X = self.df.drop(columns=[self.target_column])
        y = self.df[self.target_column]

        # Handle non-numeric columns
        X = pd.get_dummies(X, drop_first=True)

        # Apply scaling
        scaler_choice = self.scale_var.get()
        if scaler_choice == "StandardScaler":
            scaler = StandardScaler()
            X = scaler.fit_transform(X)
        elif scaler_choice == "MinMaxScaler":
            scaler = MinMaxScaler()
            X = scaler.fit_transform(X)

        # Train-test split
        X_train, self.X_test, y_train, self.y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        # Apply SMOTE
        if self.balance_var.get():
            smote = SMOTE(random_state=42)
            X_train, y_train = smote.fit_resample(X_train, y_train)

        model_type = self.model_var.get()
        if model_type == "Logistic Regression":
            self.model = LogisticRegression(max_iter=1000, random_state=0)
        elif model_type == "Decision Tree":
            self.model = DecisionTreeClassifier(random_state=0)
        elif model_type == "Random Forest":
            self.model = RandomForestClassifier(random_state=0)
        elif model_type == "XGBoost":
            self.model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=0)
        else:
            messagebox.showerror("Error", "Please select a valid model.")
            return

        self.model.fit(X_train, y_train)
        y_pred = self.model.predict(self.X_test)
        y_prob = self.model.predict_proba(self.X_test)[:, 1]

        # Plotting
        self.plot_metrics(self.y_test, y_pred, self.model, self.X_test, 0, 0)
        self.plot_cv_results(X_train, y_train,self.X_test, self.y_test, 0, 1)
        self.plot_confusion_matrix(self.y_test, y_pred, 1, 0)
        self.plot_feature_importance(1, 1)




    def plot_metrics(self, y_test, y_pred, model, X_test, row, col):
        # Evaluate metrics
        train_score = model.score(X_test, y_test)  # Using test set for simplicity
        test_score = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)

        # Prepare metrics dictionary
        metrics = {
            "Model": model.__class__.__name__,
            "Training Score": train_score,
            "Test Score": test_score,
            "Precision": precision,
            "Recall": recall,
            "F1 Score": f1
        }

        # Create a styled figure
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.axis('off')
        ax.set_facecolor('#ecf0f1')

        # Style settings
        title_font = {'fontsize': 16, 'fontweight': 'bold', 'color': '#2c3e50'}
        text_font = {'fontsize': 13, 'color': '#34495e'}

        # Title
        ax.text(0.5, 0.85, metrics["Model"], ha='center', **title_font, transform=ax.transAxes)

        # Metrics display
        spacing = 0.70
        for key in list(metrics.keys())[1:]:
            ax.text(0.5, spacing, f"{key}: {metrics[key]:.2f}", ha='center', **text_font, transform=ax.transAxes)
            spacing -= 0.10

        plt.tight_layout()

        # Show the figure directly in tkinter
        self.show_plot(fig, row, col)
            

    def plot_cv_results(self, X_train, y_train, X_test, y_test, row, col):
        model_type = self.model_var.get()

        # Select model
        if model_type == "Logistic Regression":
            model = LogisticRegression(max_iter=1000)
        elif model_type == "Decision Tree":
            model = DecisionTreeClassifier()
        elif model_type == "Random Forest":
            model = RandomForestClassifier()
        elif model_type == "XGBoost":
            model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
        else:
            return

        # 5-Fold Cross-Validation
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')
        avg_cv = np.mean(cv_scores)
        std_cv = np.std(cv_scores)

        # Train and predict for classification report
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        # Get classification report
        target_names = list(map(str, np.unique(y_test)))
        report = classification_report(y_test, y_pred, target_names=target_names)
        report_lines = report.strip().split('\n')

        # Add 2-tab indent to the header line (assumes it's line 0)
        if len(report_lines) > 0:
            report_lines[0] = '        ' * 2 + report_lines[0]  # 16 spaces

        # Build plot
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.axis('off')
        ax.set_facecolor('#ecf0f1')

        # Fonts
        title_font = {'fontsize': 15, 'fontweight': 'bold', 'color': '#2c3e50'}
        summary_font = {'fontsize': 11, 'color': '#16a085'}
        text_font = {'fontsize': 11, 'color': '#34495e', 'family': 'monospace'}

        # Header
        ax.text(0.5, 1.02, f'{model_type} - CV and Classification Report', ha='center', **title_font, transform=ax.transAxes)
        ax.text(0.5, 0.97, f'5-Fold CV Avg Accuracy: {avg_cv:.4f} | Std Dev: {std_cv:.4f}', ha='center', **summary_font, transform=ax.transAxes)

        # Classification report lines
        y_pos = 0.88
        for line in report_lines:
            ax.text(0.01, y_pos, line, ha='left', **text_font, transform=ax.transAxes)
            y_pos -= 0.045

        plt.tight_layout()
        self.show_plot(fig, row, col)

    def plot_confusion_matrix(self, y_test, y_pred, row, col):
        cm = confusion_matrix(y_test, y_pred)
        fig, ax = plt.subplots()
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                    xticklabels=['Not Churned', 'Churned'], yticklabels=['Not Churned', 'Churned'])
        ax.set_title('Confusion Matrix')
        ax.set_xlabel('Predicted Label')
        ax.set_ylabel('True Label')
        self.show_plot(fig, row, col)

    def plot_feature_importance(self, row, col):
        model_type = self.model_var.get()
        if model_type == "Logistic Regression":
            if hasattr(self.model, 'coef_'):
                importance = self.model.coef_[0]
                feature_names = self.X_test.columns.tolist()
                feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': np.abs(importance)})
                feature_importance = feature_importance.sort_values(by='Importance', ascending=False).head(10).iloc[::-1]

                fig, ax = plt.subplots()
                ax.barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')
                ax.set_title('Top 10 Feature Importance (Logistic Regression)')
                ax.set_xlabel('Coefficient Magnitude')
                ax.set_ylabel('Feature')
                self.show_plot(fig, row, col)
            else:
                # Handle cases where feature importance is not directly available
                fig, ax = plt.subplots()
                ax.text(0.5, 0.5, 'Feature importance not available for Logistic Regression without fitted coefficients.', ha='center', va='center')
                self.show_plot(fig, row, col)

        elif hasattr(self.model, 'feature_importances_'):
            importance = self.model.feature_importances_
            feature_names = self.X_test.columns.tolist()
            feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
            feature_importance = feature_importance.sort_values(by='Importance', ascending=False).head(10).iloc[::-1]

            fig, ax = plt.subplots()
            ax.barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')
            ax.set_title(f'Top 10 Feature Importance ({model_type})')
            ax.set_xlabel('Importance')
            ax.set_ylabel('Feature')
            self.show_plot(fig, row, col)
        else:
            fig, ax = plt.subplots()
            ax.text(0.5, 0.5, f'Feature importance not available for {model_type}.', ha='center', va='center')
            self.show_plot(fig, row, col)


    def show_plot(self, fig, row, col):
        if self.canvases[row][col]:
            self.canvases[row][col].get_tk_widget().destroy()
        canvas = FigureCanvasTkAgg(fig, master=self.canvas_frames[row][col])
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        self.canvases[row][col] = canvas

if __name__ == '__main__':
    root = tk.Tk()
    app = ModelTrainerApp(root)
    root.mainloop()