{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Case for Building a Predictive Model to Understand Employee Attrition\n",
    "\n",
    "#### 1. Business Understanding\n",
    "\n",
    "**1.1 Determine Business Objectives**\n",
    "The primary objective is to reduce employee attrition rates by understanding the key characteristics that influence why employees leave the firm. High attrition rates can lead to increased recruitment and training costs, loss of organizational knowledge, and reduced employee morale.\n",
    "\n",
    "**1.2 Assess Situation**\n",
    "- **Current State:** The firm is experiencing an annual attrition rate of 16%, which is higher than the industry average of 12%.\n",
    "- **Resources Available:**\n",
    "  - **Data:** Employee demographics, job roles, tenure, performance ratings, salary information, and exit interviews.\n",
    "  - **People:** HR team, data analysts, and IT support.\n",
    "  - **Technology:** Data storage systems, data analysis software, and computing resources.\n",
    "\n",
    "**1.3 Determine Data Mining Goals**\n",
    "- Develop a predictive model to identify the factors that contribute to employee attrition.\n",
    "- Use the model to predict which employees are at a high risk of leaving.\n",
    "- Provide actionable insights to the HR department to develop targeted retention strategies.\n",
    "\n",
    "**1.4 Produce Project Plan**\n",
    "- **Phase 1:** Data Collection and Understanding (2 weeks)\n",
    "- **Phase 2:** Data Preparation (3 weeks)\n",
    "- **Phase 3:** Modeling (4 weeks)\n",
    "- **Phase 4:** Evaluation (2 weeks)\n",
    "- **Phase 5:** Deployment (3 weeks)\n",
    "- **Phase 6:** Monitoring and Maintenance (ongoing)\n",
    "\n",
    "#### 2. Data Understanding\n",
    "\n",
    "**2.1 Collect Initial Data**\n",
    "- Gather data from HR databases, including employee demographics, job history, performance reviews, and exit interview feedback.\n",
    "\n",
    "**2.2 Describe Data**\n",
    "- Data set includes attributes such as age, gender, education level, job role, tenure, performance rating, salary, and whether the employee has left the firm.\n",
    "\n",
    "**2.3 Explore Data**\n",
    "- Use statistical methods and visualization tools to identify patterns and relationships within the data.\n",
    "\n",
    "**2.4 Verify Data Quality**\n",
    "- Check for missing values, duplicates, and inconsistencies. Ensure data accuracy and completeness.\n",
    "\n",
    "#### 3. Data Preparation\n",
    "\n",
    "**3.1 Select Data**\n",
    "- Identify relevant attributes such as age, job role, tenure, and performance rating.\n",
    "\n",
    "**3.2 Clean Data**\n",
    "- Handle missing values, correct errors, and remove duplicates.\n",
    "\n",
    "**3.3 Construct Data**\n",
    "- Create new features if necessary, such as tenure categories or performance trends.\n",
    "\n",
    "**3.4 Integrate Data**\n",
    "- Combine data from different sources to create a comprehensive data set.\n",
    "\n",
    "**3.5 Format Data**\n",
    "- Organize the data into a structure suitable for modeling, such as a clean and normalized table.\n",
    "\n",
    "#### 4. Modeling\n",
    "\n",
    "**4.1 Select Modeling Techniques**\n",
    "- Choose techniques such as logistic regression, decision trees, and random forests.\n",
    "\n",
    "**4.2 Generate Test Design**\n",
    "- Split data into training and test sets to evaluate model performance.\n",
    "\n",
    "**4.3 Build Model**\n",
    "- Apply selected modeling techniques to the training data to build the predictive models.\n",
    "\n",
    "*Models Built*\n",
    "1. Logistic Regression Model\n",
    "2. Discriminant Analysis\n",
    "  Linear and Quadratic\n",
    "3. Desccision Tree\n",
    "4. Random Forest\n",
    "5. XGBoost\n",
    "\n",
    "**4.4 Assess Model**\n",
    "- Evaluate model performance using metrics such as accuracy, precision, recall, and ROC-AUC.\n",
    "\n",
    "#### 5. Evaluation\n",
    "\n",
    "**5.1 Evaluate Results**\n",
    "- Assess the model's performance in predicting employee attrition. Ensure it meets the business objectives.\n",
    "\n",
    "**5.2 Review Process**\n",
    "- Review all steps taken to ensure they align with the goals and that the methodology was correctly applied.\n",
    "\n",
    "**5.3 Determine Next Steps**\n",
    "- Decide whether to proceed with model deployment, make adjustments to the model, or conduct further iterations.\n",
    "\n",
    "#### 6. Deployment\n",
    "\n",
    "**6.1 Plan Deployment**\n",
    "- Develop a strategy to integrate the predictive model into the HR systems for ongoing use.\n",
    "\n",
    "**6.2 Monitor and Maintain**\n",
    "- Set up regular monitoring to track the model’s performance and update it as necessary.\n",
    "\n",
    "**6.3 Review Project**\n",
    "- Conduct a final review to document the project’s successes and areas for improvement.\n",
    "\n",
    "**6.4 Produce Final Report**\n",
    "- Create a detailed report summarizing the project, including findings, model performance, and recommendations.\n",
    "\n",
    "**6.5 Presentation**\n",
    "- Present the results and recommendations to the stakeholders, including the HR team and senior management.\n",
    "\n",
    "### Summary\n",
    "\n",
    "By following the CRISP-DM methodology, the firm aims to develop a robust predictive model that will help understand and address the factors influencing employee attrition. This will lead to targeted retention strategies, reduced turnover rates, and improved organizational stability and morale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mCreated on Thu June 15 17:37:49 2024\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[33;03m@author: Sumaila\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Libraries\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Needed for data i/o\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu June 15 17:37:49 2024\n",
    "\n",
    "@author: Sumaila\n",
    "\"\"\"\n",
    "\n",
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "# Needed for data i/o\n",
    "import numpy as np\n",
    "# Needed for linear algebra operations\n",
    "import pickle\n",
    "# Needed for model export\n",
    "import seaborn as sns\n",
    "# Needed for data visualisation\n",
    "from scipy.stats import ttest_ind,randint\n",
    "# Needed for T-test\n",
    "import matplotlib.pyplot as plt\n",
    "# Needed for data visualisation\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "# Needed for text to image conversion\n",
    "from sklearn import tree\n",
    "# Needed for decision tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Needed for logistic regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Needed for random forest\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# Needed for discriminant analysis\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# TODO: Why do we need this.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# TODO: Why do we need this.\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# TODO: Why do we need this.\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "# Needed for decision tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Needed for train-test split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Needed for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Needed for Data Preprocessing. ie: Standardized Scaling\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Needed for confusion matrix. ie: model accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "# Needed for classification report. ie: precision, recall, f1-score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# Needed for parameterization. ie. determining the best set of parameters\n",
    "# that optimizes the model outcome.\n",
    "from scipy import stats\n",
    "# Needed for Chi Squared test\n",
    "from sklearn import metrics\n",
    "# Needed for model evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Needed for model cross validation.\n",
    "from google.colab import drive\n",
    "# Needed for importing data from google drive\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Needed for ignoring warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
